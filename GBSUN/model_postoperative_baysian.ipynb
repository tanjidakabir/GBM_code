{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "import os \n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from random import choice\n",
    "import numpy as np \n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from monai.losses import GeneralizedDiceLoss,DiceCELoss,DiceFocalLoss,GeneralizedDiceFocalLoss\n",
    "from monai.metrics import compute_meandice\n",
    "from monai.networks.blocks import Convolution, ResidualUnit\n",
    "from collections import OrderedDict\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from bayesian_torch.layers import Conv3dFlipout\n",
    "from bayesian_torch.models.dnn_to_bnn import dnn_to_bnn, get_kl_loss\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_path='./pre_process_data/train/train_index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df=pd.read_csv(data_df_path)\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "data_df=pd.read_csv(data_df_path)\n",
    "data_df['index']=range(len(data_df))\n",
    "train_df=data_df.sample(frac=0.9)\n",
    "val_df=data_df[~data_df['index'].isin(train_df['index'])]\n",
    "print(len(train_df))\n",
    "print(len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpletrain(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.data = dataset\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data_index = self.data.iloc[idx]\n",
    "        images_path=data_index.images_path\n",
    "        seg_path=data_index.seg_path\n",
    "        with open(images_path, 'rb') as f:\n",
    "            images = pickle.load(f)\n",
    "        \n",
    "        with open(seg_path, 'rb') as f:\n",
    "            seg = pickle.load(f)\n",
    "        \n",
    "        \n",
    "        return images, seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Double_conv_encoder_block_in (num_in_filter,args):\n",
    "    input_dim=args['input_dim'] ; kernal_size=args['kernal_size']; pool_kernel_size=args['pool_kernal_size'];\n",
    "    dilation_size=args['dilation_size']; stride_size=args['stride_size'] ; pool_stride_size=args['pool_stride_size'];\n",
    "    padding_size=args['padding_size']; bias=args['bias'] ; act_type= args['act_type'] ; norm_type=args['norm_type'] ; \n",
    "    pool_type=args['pool_type'] ;\n",
    "    \n",
    "    conv1=Convolution(input_dim,num_in_filter,num_in_filter*2, strides=stride_size, \n",
    "                      kernel_size=kernal_size,  dilation=dilation_size,\n",
    "                         bias=bias, conv_only=True,  padding=padding_size)\n",
    "    conv2=Convolution(input_dim,num_in_filter*2,num_in_filter*4, strides=stride_size, \n",
    "                      kernel_size=kernal_size,  dilation=dilation_size,\n",
    "                         bias=bias, conv_only=True,  padding=padding_size)\n",
    "    \n",
    "    if act_type=='PReLU':\n",
    "        act_layer=nn.PReLU()\n",
    "    elif act_type=='SELU':\n",
    "        act_layer=nn.SELU()\n",
    "    elif act_type=='CELU':\n",
    "        act_layer=nn.CELU()\n",
    "    else:\n",
    "        act_layer=nn.ReLU()\n",
    "    \n",
    "    if norm_type=='GroupNorm':\n",
    "        norm_layer_1=nn.GroupNorm(8,num_in_filter*2)\n",
    "        norm_layer_2=nn.GroupNorm(8,num_in_filter*4)\n",
    "    elif act_type=='BatchNorm':\n",
    "        norm_layer_1=nn.LazyBatchNorm3d()\n",
    "        norm_layer_2=nn.LazyBatchNorm3d()\n",
    "    else: \n",
    "        norm_layer_1=nn.LazyInstanceNorm3d()\n",
    "        norm_layer_2=nn.LazyInstanceNorm3d()\n",
    "    \n",
    "    encoder_block=nn.Sequential(OrderedDict(\n",
    "        [('conv1', conv1),\n",
    "        (act_type+'_1', act_layer),\n",
    "        (norm_type+'_1', norm_layer_1),\n",
    "        ('conv2', conv2),\n",
    "        (act_type+'_2', act_layer),\n",
    "        (norm_type+'_2', norm_layer_2)]))\n",
    "    \n",
    "    return encoder_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Double_conv_encoder_block_en (num_in_filter,args):\n",
    "    input_dim=args['input_dim'] ; kernal_size=args['kernal_size']; pool_kernel_size=args['pool_kernal_size'];\n",
    "    dilation_size=args['dilation_size']; stride_size=args['stride_size'] ; pool_stride_size=args['pool_stride_size'];\n",
    "    padding_size=args['padding_size']; bias=args['bias'] ; act_type= args['act_type'] ; norm_type=args['norm_type'] ; \n",
    "    pool_type=args['pool_type'] ;\n",
    "    \n",
    "    assert pool_type in ['max', 'avg']\n",
    "    if pool_type == 'avg':\n",
    "        pooling = nn.AvgPool3d(kernel_size=pool_kernel_size, stride=pool_stride_size, padding=0, dilation=1, ceil_mode=False)\n",
    "    else:\n",
    "        pooling = nn.MaxPool3d(kernel_size=pool_kernel_size, stride=pool_stride_size, padding=0, dilation=1, ceil_mode=False)\n",
    "      \n",
    "    \n",
    "    conv1=Convolution(input_dim,num_in_filter*2,num_in_filter*2, strides=stride_size, \n",
    "                      kernel_size=kernal_size,  dilation=dilation_size,\n",
    "                         bias=bias, conv_only=True,  padding=padding_size)\n",
    "    conv2=Convolution(input_dim,num_in_filter*2,num_in_filter*4, strides=stride_size, \n",
    "                      kernel_size=kernal_size,  dilation=dilation_size,\n",
    "                         bias=bias, conv_only=True,  padding=padding_size)\n",
    "    \n",
    "    if act_type=='PReLU':\n",
    "        act_layer=nn.PReLU()\n",
    "    elif act_type=='SELU':\n",
    "        act_layer=nn.SELU()\n",
    "    elif act_type=='CELU':\n",
    "        act_layer=nn.CELU()\n",
    "    else:\n",
    "        act_layer=nn.ReLU()\n",
    "    \n",
    "    if norm_type=='GroupNorm':\n",
    "        norm_layer_1=nn.GroupNorm(8,num_in_filter*2)\n",
    "        norm_layer_2=nn.GroupNorm(8,num_in_filter*4)\n",
    "    elif act_type=='BatchNorm':\n",
    "        norm_layer_1=nn.LazyBatchNorm3d()\n",
    "        norm_layer_2=nn.LazyBatchNorm3d()\n",
    "    else: \n",
    "        norm_layer_1=nn.LazyInstanceNorm3d()\n",
    "        norm_layer_2=nn.LazyInstanceNorm3d()\n",
    "    \n",
    "    encoder_block=nn.Sequential(OrderedDict(\n",
    "        [('pool', pooling),\n",
    "        ('conv1', conv1),\n",
    "        (act_type+'_1', act_layer),\n",
    "        (norm_type+'_1', norm_layer_1),\n",
    "        ('conv2', conv2),\n",
    "        (act_type+'_2', act_layer),\n",
    "        (norm_type+'_2', norm_layer_2)]))\n",
    "    \n",
    "    return encoder_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Double_conv_encoder_block_de(nn.Module):\n",
    "    def __init__(self, num_in_filter,args):\n",
    "        super(Double_conv_encoder_block_de, self).__init__()\n",
    "        self.args=args\n",
    "        self.num_in_filter=num_in_filter\n",
    "        input_dim=self.args['input_dim'] ; kernal_size=self.args['kernal_size']; \n",
    "        pool_kernel_size=self.args['pool_kernal_size'];dilation_size=self.args['dilation_size']; \n",
    "        stride_size=self.args['stride_size'] ; pool_stride_size=self.args['pool_stride_size'];\n",
    "        padding_size=self.args['padding_size']; bias=self.args['bias'] ; act_type= self.args['act_type'] ; \n",
    "        norm_type=self.args['norm_type'] ; pool_type=self.args['pool_type'] ; baysian=self.args['baysian'] ; \n",
    "        prior_mean=self.args['baysian_prior_mean'];prior_variance=self.args['baysian_prior_variance'] ; \n",
    "        posterior_mu_init=self.args['baysian_post_mu']; posterior_rho_init=self.args['baysian_post_rho']\n",
    "        \n",
    "        if baysian==True:\n",
    "            self.conv1=Conv3dFlipout(num_in_filter,int(num_in_filter/3),kernel_size=kernal_size ,stride=stride_size,  dilation=dilation_size,\n",
    "                         bias=bias,  padding=padding_size, \n",
    "                         prior_mean=prior_mean, prior_variance=prior_variance, \n",
    "                         posterior_mu_init=posterior_mu_init, posterior_rho_init=posterior_rho_init)\n",
    "            self.conv2=Conv3dFlipout(int(num_in_filter/3),int(num_in_filter/3),kernel_size=kernal_size, stride=stride_size, \n",
    "                        dilation=dilation_size,\n",
    "                         bias=bias, padding=padding_size,\n",
    "                        prior_mean=prior_mean, prior_variance=prior_variance, \n",
    "                         posterior_mu_init=posterior_mu_init, posterior_rho_init=posterior_rho_init)\n",
    "        else: \n",
    "    \n",
    "            self.conv1=Convolution(input_dim,num_in_filter,int(num_in_filter/3), strides=stride_size, \n",
    "                          kernel_size=kernal_size,  dilation=dilation_size,\n",
    "                             bias=bias, conv_only=True,  padding=padding_size)\n",
    "            self.conv2=Convolution(input_dim,int(num_in_filter/3),int(num_in_filter/3), strides=stride_size, \n",
    "                          kernel_size=kernal_size,  dilation=dilation_size,\n",
    "                             bias=bias, conv_only=True,  padding=padding_size)\n",
    "\n",
    "        if act_type=='PReLU':\n",
    "            self.act_layer=nn.PReLU()\n",
    "        elif act_type=='SELU':\n",
    "            self.act_layer=nn.SELU()\n",
    "        elif act_type=='CELU':\n",
    "            self.act_layer=nn.CELU()\n",
    "        else:\n",
    "            self.act_layer=nn.ReLU()\n",
    "    \n",
    "        if norm_type=='GroupNorm':\n",
    "            self.norm_layer_1=nn.GroupNorm(8,int(num_in_filter/3))\n",
    "            self.norm_layer_2=nn.GroupNorm(8,int(num_in_filter/3))\n",
    "        elif act_type=='BatchNorm':\n",
    "            self.norm_layer_1=nn.LazyBatchNorm3d()\n",
    "            self.norm_layer_2=nn.LazyBatchNorm3d()\n",
    "        else: \n",
    "            self.norm_layer_1=nn.LazyInstanceNorm3d()\n",
    "            self.norm_layer_2=nn.LazyInstanceNorm3d()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        kl_sum = 0\n",
    "        x, kl = self.conv1(x)\n",
    "        kl_sum += kl\n",
    "        x = self.act_layer(x)\n",
    "        x = self.norm_layer_1(x)\n",
    "        x, kl = self.conv2(x)\n",
    "        kl_sum += kl\n",
    "        x = self.act_layer(x)\n",
    "        x = self.norm_layer_2(x)\n",
    "    \n",
    "        return x , kl_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_3d_encoder(nn.Module):\n",
    "    def __init__(self, args,en_channel):\n",
    "        super(UNet_3d_encoder, self).__init__()\n",
    "        self.args=args\n",
    "        encoders = []\n",
    "        \n",
    "        input_en=Double_conv_encoder_block_in (en_channel[0],self.args)\n",
    "        encoders.append(input_en)\n",
    "        en_channel.pop(0)\n",
    "        for i in en_channel:\n",
    "            encoder=Double_conv_encoder_block_en (i,self.args)\n",
    "            encoders.append(encoder)\n",
    "        self.encoders=nn.ModuleList(encoders)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encoder part\n",
    "        encoders_features = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "            # reverse the encoder outputs to be aligned with the decoder\n",
    "            encoders_features.insert(0, x)\n",
    "        final_layer= encoders_features[0]\n",
    "        encoders_features_o = encoders_features[1:]\n",
    "        \n",
    "        return final_layer , encoders_features_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_3d_decoder(nn.Module):\n",
    "    def __init__(self, args,de_channel):\n",
    "        super(UNet_3d_decoder, self).__init__()\n",
    "        self.args=args\n",
    "        decoders = []\n",
    "        \n",
    "        for i in de_channel:\n",
    "            decoder=Double_conv_encoder_block_de (i,self.args)\n",
    "            decoders.append(decoder)\n",
    "        self.decoders=nn.ModuleList(decoders)\n",
    "        \n",
    "        \n",
    "    def forward(self, x,encoders_features):\n",
    "        # encoder part\n",
    "        all_kl=0\n",
    "        for decoder, encoder_features in zip(self.decoders, encoders_features):\n",
    "            output_size = encoder_features.size()[2:]\n",
    "            \n",
    "            x = F.interpolate(x, size=output_size, mode='nearest')\n",
    "            \n",
    "            # concatenate encoder_features (encoder path) with the upsampled input across channel dimension\n",
    "            x = torch.cat((encoder_features, x), dim=1)\n",
    "            \n",
    "            x,kl=decoder(x)\n",
    "            all_kl+=kl\n",
    "        \n",
    "        \n",
    "        return x,kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_maps(init_channel_number, number_of_layers):\n",
    "    return [init_channel_number * 2 ** k for k in range(number_of_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder_maps(init_channel_number,encoder_maps_list_r):\n",
    "    decoder_maps=[]\n",
    "    for i in range(len(encoder_maps_list_r)):\n",
    "        try:\n",
    "            decoder_map=encoder_maps_list_r[i]*init_channel_number+encoder_maps_list_r[i+1]*init_channel_number\n",
    "            decoder_maps.append(decoder_map)\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "    return decoder_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, encoder,decoder,encoder_pre,decoder_pre, args):\n",
    "        super(Unet, self).__init__()\n",
    "        self.args=args\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.encoder_pre = encoder_pre\n",
    "        self.decoder_pre= decoder_pre\n",
    "        \n",
    "        self.final_conv = nn.Conv3d(self.args['class_nu']**2,self.args['class_nu'], kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
    "        \n",
    "        \n",
    "    def forward(self, images ):\n",
    "        final_layer , encoders_features_o=self.encoder(images)\n",
    "        final_layer_pre , encoders_features_o_pre=self.encoder_pre(images)\n",
    "        \n",
    "        out,kl = self.decoder(final_layer , encoders_features_o)\n",
    "       \n",
    "        out_pre,kl_pre = self.decoder_pre(final_layer_pre , encoders_features_o_pre)\n",
    "        \n",
    "        out=out+out_pre\n",
    "        out=self.final_conv(out)\n",
    "       \n",
    "        \n",
    "        return out,kl+kl_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_as_one_hot(input, C, ignore_index=None):\n",
    "    \"\"\"\n",
    "    Converts NxDxHxW label image to NxCxDxHxW, where each label gets converted to its corresponding one-hot vector\n",
    "    :param input: 4D input image (NxDxHxW)\n",
    "    :param C: number of channels/labels\n",
    "    :param ignore_index: ignore index to be kept during the expansion\n",
    "    :return: 5D output image (NxCxDxHxW)\n",
    "    \"\"\"\n",
    "    assert input.dim() == 4\n",
    "\n",
    "    # expand the input tensor to Nx1xDxHxW before scattering\n",
    "    input = input.unsqueeze(1)\n",
    "    # create result tensor shape (NxCxDxHxW)\n",
    "    shape = list(input.size())\n",
    "    shape[1] = C\n",
    "\n",
    "    if ignore_index is not None:\n",
    "        # create ignore_index mask for the result\n",
    "        mask = input.expand(shape) == ignore_index\n",
    "        # clone the src tensor and zero out ignore_index in the input\n",
    "        input = input.clone()\n",
    "        input[input == ignore_index] = 0\n",
    "        # scatter to get the one-hot tensor\n",
    "        result = torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n",
    "        # bring back the ignore_index in the result\n",
    "        result[mask] = ignore_index\n",
    "        return result\n",
    "    else:\n",
    "        # scatter to get the one-hot tensor\n",
    "        return torch.zeros(shape).to(input.device).scatter_(1, input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_en_path_pre='./model/3d_unet_preoperative/3DUNet_preoperative_encoder_baysian.pt'\n",
    "load_de_path_pre='./model/3d_unet_preoperative/3DUNet_preoperative_decoder_baysian.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={}\n",
    "args['epoch']=30\n",
    "args['input_dim']=3\n",
    "args['kernal_size']=3\n",
    "args['slice_nu']=155\n",
    "args['window_size']=10\n",
    "args['pool_kernal_size']=2\n",
    "args['dilation_size']=1\n",
    "args['stride_size']=1\n",
    "args['pool_stride_size']=2\n",
    "args['padding_size']=1\n",
    "args['bias']=True\n",
    "args['act_type']='PReLU'\n",
    "args['norm_type']='GroupNorm'\n",
    "args['pool_type']='max'\n",
    "args['class_nu']=4\n",
    "args['batch_size']=2\n",
    "args['baysian']=True ; \n",
    "args['baysian_prior_mean']=0.0\n",
    "args['baysian_prior_variance']=1.0\n",
    "args['baysian_post_mu']=0\n",
    "args['baysian_post_rho']=-3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 16, 32]\n"
     ]
    }
   ],
   "source": [
    "en_channel=create_encoder_maps(4, 4)\n",
    "en_channel_r=en_channel.copy()\n",
    "en_channel_r.reverse()\n",
    "print(en_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[192, 96, 48]\n"
     ]
    }
   ],
   "source": [
    "de_channel=create_decoder_maps(4,en_channel_r)\n",
    "print(de_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=14\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_channel_p=en_channel.copy()\n",
    "encoder=UNet_3d_encoder(args,en_channel_p)\n",
    "de_channel_p=de_channel.copy()\n",
    "decoder=UNet_3d_decoder(args,de_channel_p)\n",
    "en_channel_p=en_channel.copy()\n",
    "encoder_pre=UNet_3d_encoder(args,en_channel_p)\n",
    "de_channel_p=de_channel.copy()\n",
    "decoder_pre=UNet_3d_decoder(args,de_channel_p)\n",
    "\n",
    "model=Unet(encoder,decoder,encoder_pre,decoder_pre,args)\n",
    "#model.encoder.load_state_dict(torch.load(load_en_path_pre))\n",
    "#model.decoder.load_state_dict(torch.load(load_de_path_pre))\n",
    "model.encoder_pre.load_state_dict(torch.load(load_en_path_pre))\n",
    "model.decoder_pre.load_state_dict(torch.load(load_de_path_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder\n",
      "decoder\n",
      "encoder_pre\n",
      "Layer \"0\" in module \"encoder_pre\" was frozen!\n",
      "decoder_pre\n",
      "Layer \"1\" in module \"decoder_pre\" was frozen!\n",
      "final_conv\n"
     ]
    }
   ],
   "source": [
    "layer_counter = 0\n",
    "for (name, module) in model.named_children():\n",
    "    print(name)\n",
    "    if '_pre' in name :\n",
    "        for layer in module.children():\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            print('Layer \"{}\" in module \"{}\" was frozen!'.format(layer_counter, name))\n",
    "            layer_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_layer_encoder.encoders.0.conv1.conv.weight\n",
      "train_layer_encoder.encoders.0.conv1.conv.bias\n",
      "train_layer_encoder.encoders.0.PReLU_1.weight\n",
      "train_layer_encoder.encoders.0.GroupNorm_1.weight\n",
      "train_layer_encoder.encoders.0.GroupNorm_1.bias\n",
      "train_layer_encoder.encoders.0.conv2.conv.weight\n",
      "train_layer_encoder.encoders.0.conv2.conv.bias\n",
      "train_layer_encoder.encoders.0.GroupNorm_2.weight\n",
      "train_layer_encoder.encoders.0.GroupNorm_2.bias\n",
      "train_layer_encoder.encoders.1.conv1.conv.weight\n",
      "train_layer_encoder.encoders.1.conv1.conv.bias\n",
      "train_layer_encoder.encoders.1.PReLU_1.weight\n",
      "train_layer_encoder.encoders.1.GroupNorm_1.weight\n",
      "train_layer_encoder.encoders.1.GroupNorm_1.bias\n",
      "train_layer_encoder.encoders.1.conv2.conv.weight\n",
      "train_layer_encoder.encoders.1.conv2.conv.bias\n",
      "train_layer_encoder.encoders.1.GroupNorm_2.weight\n",
      "train_layer_encoder.encoders.1.GroupNorm_2.bias\n",
      "train_layer_encoder.encoders.2.conv1.conv.weight\n",
      "train_layer_encoder.encoders.2.conv1.conv.bias\n",
      "train_layer_encoder.encoders.2.PReLU_1.weight\n",
      "train_layer_encoder.encoders.2.GroupNorm_1.weight\n",
      "train_layer_encoder.encoders.2.GroupNorm_1.bias\n",
      "train_layer_encoder.encoders.2.conv2.conv.weight\n",
      "train_layer_encoder.encoders.2.conv2.conv.bias\n",
      "train_layer_encoder.encoders.2.GroupNorm_2.weight\n",
      "train_layer_encoder.encoders.2.GroupNorm_2.bias\n",
      "train_layer_encoder.encoders.3.conv1.conv.weight\n",
      "train_layer_encoder.encoders.3.conv1.conv.bias\n",
      "train_layer_encoder.encoders.3.PReLU_1.weight\n",
      "train_layer_encoder.encoders.3.GroupNorm_1.weight\n",
      "train_layer_encoder.encoders.3.GroupNorm_1.bias\n",
      "train_layer_encoder.encoders.3.conv2.conv.weight\n",
      "train_layer_encoder.encoders.3.conv2.conv.bias\n",
      "train_layer_encoder.encoders.3.GroupNorm_2.weight\n",
      "train_layer_encoder.encoders.3.GroupNorm_2.bias\n",
      "train_layer_decoder.decoders.0.conv1.mu_kernel\n",
      "train_layer_decoder.decoders.0.conv1.rho_kernel\n",
      "train_layer_decoder.decoders.0.conv1.mu_bias\n",
      "train_layer_decoder.decoders.0.conv1.rho_bias\n",
      "train_layer_decoder.decoders.0.conv2.mu_kernel\n",
      "train_layer_decoder.decoders.0.conv2.rho_kernel\n",
      "train_layer_decoder.decoders.0.conv2.mu_bias\n",
      "train_layer_decoder.decoders.0.conv2.rho_bias\n",
      "train_layer_decoder.decoders.0.act_layer.weight\n",
      "train_layer_decoder.decoders.0.norm_layer_1.weight\n",
      "train_layer_decoder.decoders.0.norm_layer_1.bias\n",
      "train_layer_decoder.decoders.0.norm_layer_2.weight\n",
      "train_layer_decoder.decoders.0.norm_layer_2.bias\n",
      "train_layer_decoder.decoders.1.conv1.mu_kernel\n",
      "train_layer_decoder.decoders.1.conv1.rho_kernel\n",
      "train_layer_decoder.decoders.1.conv1.mu_bias\n",
      "train_layer_decoder.decoders.1.conv1.rho_bias\n",
      "train_layer_decoder.decoders.1.conv2.mu_kernel\n",
      "train_layer_decoder.decoders.1.conv2.rho_kernel\n",
      "train_layer_decoder.decoders.1.conv2.mu_bias\n",
      "train_layer_decoder.decoders.1.conv2.rho_bias\n",
      "train_layer_decoder.decoders.1.act_layer.weight\n",
      "train_layer_decoder.decoders.1.norm_layer_1.weight\n",
      "train_layer_decoder.decoders.1.norm_layer_1.bias\n",
      "train_layer_decoder.decoders.1.norm_layer_2.weight\n",
      "train_layer_decoder.decoders.1.norm_layer_2.bias\n",
      "train_layer_decoder.decoders.2.conv1.mu_kernel\n",
      "train_layer_decoder.decoders.2.conv1.rho_kernel\n",
      "train_layer_decoder.decoders.2.conv1.mu_bias\n",
      "train_layer_decoder.decoders.2.conv1.rho_bias\n",
      "train_layer_decoder.decoders.2.conv2.mu_kernel\n",
      "train_layer_decoder.decoders.2.conv2.rho_kernel\n",
      "train_layer_decoder.decoders.2.conv2.mu_bias\n",
      "train_layer_decoder.decoders.2.conv2.rho_bias\n",
      "train_layer_decoder.decoders.2.act_layer.weight\n",
      "train_layer_decoder.decoders.2.norm_layer_1.weight\n",
      "train_layer_decoder.decoders.2.norm_layer_1.bias\n",
      "train_layer_decoder.decoders.2.norm_layer_2.weight\n",
      "train_layer_decoder.decoders.2.norm_layer_2.bias\n",
      "freeze_layer_encoder_pre.encoders.0.conv1.conv.weight\n",
      "freeze_layer_encoder_pre.encoders.0.conv1.conv.bias\n",
      "freeze_layer_encoder_pre.encoders.0.PReLU_1.weight\n",
      "freeze_layer_encoder_pre.encoders.0.GroupNorm_1.weight\n",
      "freeze_layer_encoder_pre.encoders.0.GroupNorm_1.bias\n",
      "freeze_layer_encoder_pre.encoders.0.conv2.conv.weight\n",
      "freeze_layer_encoder_pre.encoders.0.conv2.conv.bias\n",
      "freeze_layer_encoder_pre.encoders.0.GroupNorm_2.weight\n",
      "freeze_layer_encoder_pre.encoders.0.GroupNorm_2.bias\n",
      "freeze_layer_encoder_pre.encoders.1.conv1.conv.weight\n",
      "freeze_layer_encoder_pre.encoders.1.conv1.conv.bias\n",
      "freeze_layer_encoder_pre.encoders.1.PReLU_1.weight\n",
      "freeze_layer_encoder_pre.encoders.1.GroupNorm_1.weight\n",
      "freeze_layer_encoder_pre.encoders.1.GroupNorm_1.bias\n",
      "freeze_layer_encoder_pre.encoders.1.conv2.conv.weight\n",
      "freeze_layer_encoder_pre.encoders.1.conv2.conv.bias\n",
      "freeze_layer_encoder_pre.encoders.1.GroupNorm_2.weight\n",
      "freeze_layer_encoder_pre.encoders.1.GroupNorm_2.bias\n",
      "freeze_layer_encoder_pre.encoders.2.conv1.conv.weight\n",
      "freeze_layer_encoder_pre.encoders.2.conv1.conv.bias\n",
      "freeze_layer_encoder_pre.encoders.2.PReLU_1.weight\n",
      "freeze_layer_encoder_pre.encoders.2.GroupNorm_1.weight\n",
      "freeze_layer_encoder_pre.encoders.2.GroupNorm_1.bias\n",
      "freeze_layer_encoder_pre.encoders.2.conv2.conv.weight\n",
      "freeze_layer_encoder_pre.encoders.2.conv2.conv.bias\n",
      "freeze_layer_encoder_pre.encoders.2.GroupNorm_2.weight\n",
      "freeze_layer_encoder_pre.encoders.2.GroupNorm_2.bias\n",
      "freeze_layer_encoder_pre.encoders.3.conv1.conv.weight\n",
      "freeze_layer_encoder_pre.encoders.3.conv1.conv.bias\n",
      "freeze_layer_encoder_pre.encoders.3.PReLU_1.weight\n",
      "freeze_layer_encoder_pre.encoders.3.GroupNorm_1.weight\n",
      "freeze_layer_encoder_pre.encoders.3.GroupNorm_1.bias\n",
      "freeze_layer_encoder_pre.encoders.3.conv2.conv.weight\n",
      "freeze_layer_encoder_pre.encoders.3.conv2.conv.bias\n",
      "freeze_layer_encoder_pre.encoders.3.GroupNorm_2.weight\n",
      "freeze_layer_encoder_pre.encoders.3.GroupNorm_2.bias\n",
      "freeze_layer_decoder_pre.decoders.0.conv1.mu_kernel\n",
      "freeze_layer_decoder_pre.decoders.0.conv1.rho_kernel\n",
      "freeze_layer_decoder_pre.decoders.0.conv1.mu_bias\n",
      "freeze_layer_decoder_pre.decoders.0.conv1.rho_bias\n",
      "freeze_layer_decoder_pre.decoders.0.conv2.mu_kernel\n",
      "freeze_layer_decoder_pre.decoders.0.conv2.rho_kernel\n",
      "freeze_layer_decoder_pre.decoders.0.conv2.mu_bias\n",
      "freeze_layer_decoder_pre.decoders.0.conv2.rho_bias\n",
      "freeze_layer_decoder_pre.decoders.0.act_layer.weight\n",
      "freeze_layer_decoder_pre.decoders.0.norm_layer_1.weight\n",
      "freeze_layer_decoder_pre.decoders.0.norm_layer_1.bias\n",
      "freeze_layer_decoder_pre.decoders.0.norm_layer_2.weight\n",
      "freeze_layer_decoder_pre.decoders.0.norm_layer_2.bias\n",
      "freeze_layer_decoder_pre.decoders.1.conv1.mu_kernel\n",
      "freeze_layer_decoder_pre.decoders.1.conv1.rho_kernel\n",
      "freeze_layer_decoder_pre.decoders.1.conv1.mu_bias\n",
      "freeze_layer_decoder_pre.decoders.1.conv1.rho_bias\n",
      "freeze_layer_decoder_pre.decoders.1.conv2.mu_kernel\n",
      "freeze_layer_decoder_pre.decoders.1.conv2.rho_kernel\n",
      "freeze_layer_decoder_pre.decoders.1.conv2.mu_bias\n",
      "freeze_layer_decoder_pre.decoders.1.conv2.rho_bias\n",
      "freeze_layer_decoder_pre.decoders.1.act_layer.weight\n",
      "freeze_layer_decoder_pre.decoders.1.norm_layer_1.weight\n",
      "freeze_layer_decoder_pre.decoders.1.norm_layer_1.bias\n",
      "freeze_layer_decoder_pre.decoders.1.norm_layer_2.weight\n",
      "freeze_layer_decoder_pre.decoders.1.norm_layer_2.bias\n",
      "freeze_layer_decoder_pre.decoders.2.conv1.mu_kernel\n",
      "freeze_layer_decoder_pre.decoders.2.conv1.rho_kernel\n",
      "freeze_layer_decoder_pre.decoders.2.conv1.mu_bias\n",
      "freeze_layer_decoder_pre.decoders.2.conv1.rho_bias\n",
      "freeze_layer_decoder_pre.decoders.2.conv2.mu_kernel\n",
      "freeze_layer_decoder_pre.decoders.2.conv2.rho_kernel\n",
      "freeze_layer_decoder_pre.decoders.2.conv2.mu_bias\n",
      "freeze_layer_decoder_pre.decoders.2.conv2.rho_bias\n",
      "freeze_layer_decoder_pre.decoders.2.act_layer.weight\n",
      "freeze_layer_decoder_pre.decoders.2.norm_layer_1.weight\n",
      "freeze_layer_decoder_pre.decoders.2.norm_layer_1.bias\n",
      "freeze_layer_decoder_pre.decoders.2.norm_layer_2.weight\n",
      "freeze_layer_decoder_pre.decoders.2.norm_layer_2.bias\n",
      "train_layer_final_conv.weight\n",
      "train_layer_final_conv.bias\n"
     ]
    }
   ],
   "source": [
    "freezed_num, pass_num = 0, 0\n",
    "for (name, param) in model.named_parameters():\n",
    "    #print(name)\n",
    "    if param.requires_grad == False:\n",
    "        print('freeze_layer_'+name)\n",
    "        freezed_num += 1\n",
    "    else:\n",
    "        print('train_layer_'+name)\n",
    "        pass_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters(),lr=0.005,weight_decay=1e-5)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=simpletrain(train_df)\n",
    "train_loader=DataLoader(train_data,batch_size=1,shuffle=True)\n",
    "#train_loader=DataLoader(train_data,2)\n",
    "val_data=simpletrain(val_df)\n",
    "val_loader=DataLoader(val_data,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion =criterion =GeneralizedDiceFocalLoss(softmax=True,lambda_gdl=1.0, gamma=3.0,lambda_focal=1.0,focal_weight=[1.0,1.0,1.0,7.0])\n",
    "\n",
    "#criterion =GeneralizedDiceLoss(softmax=True,include_background=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef2(y_true, y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    union = np.sum(y_true_f) + np.sum(y_pred_f)\n",
    "    if union==0: return 1\n",
    "    \n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    \n",
    "    return 2. * intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_return(newseg,indices ):\n",
    "    background_dice=[]\n",
    "    enh_dice=[]\n",
    "    nonenc_dice=[]\n",
    "    edema_dice=[]\n",
    "\n",
    "    seg = newseg\n",
    "    pred = indices\n",
    "    \n",
    "    newsegback=np.where(seg==0,1,0)\n",
    "    newpredback=np.where(pred==0,1,0)\n",
    "    bdice=dice_coef2(newsegback, newpredback)\n",
    "    background_dice.append(bdice)\n",
    "    \n",
    "        #'edema'\n",
    "    newsegback=np.where(seg==1,1,0)\n",
    "    newpredback=np.where(pred==1,1,0)\n",
    "    bdice=dice_coef2(newsegback, newpredback)\n",
    "    edema_dice.append(bdice)\n",
    "        #'enhencing'\n",
    "    newsegback=np.where(seg==2,1,0)\n",
    "    newpredback=np.where(pred==2,1,0)\n",
    "    bdice=dice_coef2(newsegback, newpredback)\n",
    "    enh_dice.append(bdice)\n",
    "        #'nonenhencing'\n",
    "    newsegback=np.where(seg==3,1,0)\n",
    "    newpredback=np.where(pred==3,1,0)\n",
    "    bdice=dice_coef2(newsegback, newpredback)\n",
    "    nonenc_dice.append(bdice)\n",
    "    print(background_dice[0],edema_dice[0],enh_dice[0],nonenc_dice[0])\n",
    "    \n",
    "    return [background_dice[0],edema_dice[0],enh_dice[0],nonenc_dice[0]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_loss=[]\n",
    "all_val_loss=[]\n",
    "\n",
    "current_val_loss = 0.0\n",
    "prev_val_loss = 100.0\n",
    "\n",
    "final_act = nn.Softmax(dim=1)\n",
    "for epoch in tqdm(range(1, args['epoch'] + 1)):\n",
    "    epoch_train_loss=0\n",
    "    for count, batch in enumerate(train_loader):\n",
    "        #print(len(batch), len(batch[0]), batch[0][0].shape, batch[0][1].shape)\n",
    "        #images, seg = batch[0][0], batch[0][1]\n",
    "        images, seg = batch\n",
    "        seg=expand_as_one_hot(seg.long(), 4, ignore_index=None)\n",
    "        images = images.cuda()\n",
    "        seg = seg.cuda()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred,train_kl_loss=model(images)\n",
    "        train_loss = criterion(pred, seg)\n",
    "        \n",
    "        train_loss=train_loss+(train_kl_loss/1)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        record_loss=train_loss.cpu().data.numpy().item()\n",
    "        \n",
    "        epoch_train_loss+=record_loss\n",
    "    scheduler.step()\n",
    "    epoch_train_loss=epoch_train_loss/count\n",
    "    print('current_train_loss {:8.5f}'.format(epoch_train_loss))\n",
    "    all_train_loss.append(epoch_train_loss)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    epoch_val_loss=0\n",
    "    eval_dice=[]\n",
    "    for count, batch in enumerate(val_loader):\n",
    "        images, seg = batch\n",
    "        seg_1=seg.cpu().detach().numpy().astype('uint8')\n",
    "        seg=expand_as_one_hot(seg.long(), 4, ignore_index=None)\n",
    "        images = images.cuda()\n",
    "        seg = seg.cuda()\n",
    "        with torch.no_grad():\n",
    "            pred,_=model(images)\n",
    "            val_loss = criterion(pred, seg)\n",
    "            record_loss=val_loss.cpu().data.numpy().item()\n",
    "            epoch_val_loss+=record_loss\n",
    "            pred = final_act(pred)\n",
    "            pred = pred.squeeze()\n",
    "            _, indices = pred.max(0)\n",
    "            indices =indices.cpu().detach().numpy().astype('uint8')\n",
    "            eval_dice.append(dice_return(indices,seg_1.squeeze()))\n",
    "            \n",
    "    eval_dsc=pd.DataFrame(eval_dice,columns=['background','flair','enh','non_enh'])       \n",
    "    epoch_val_loss=epoch_val_loss/count\n",
    "    all_val_loss.append(epoch_val_loss)\n",
    "    \n",
    "    print('current_train_loss {:8.5f},current_val_loss {:8.5f}'\n",
    "          .format(epoch_train_loss,epoch_val_loss))\n",
    "    current_val_loss = epoch_val_loss\n",
    "    \n",
    "    if current_val_loss < prev_val_loss:\n",
    "        model_name='3DUNet_postoperative_encoder_baysian_low_7'+'.pt'\n",
    "        torch.save(model.encoder.state_dict(), './model/3d_unet_postoperative/'+ model_name)\n",
    "        model_name='3DUNet_postoperative_decoder_baysian_low_7'+'.pt'\n",
    "        torch.save(model.decoder.state_dict(), './model/3d_unet_preoperative/'+ model_name)\n",
    "        model_name='3DUNet_postoperative_final_conv_baysian_low_7'+'.pt'\n",
    "        torch.save(model.final_conv.state_dict(), './model/3d_unet_postoperative/'+ model_name)\n",
    "        model_name='3DUNet_postoperative_whole_model_baysian_low_7'+'.pt'\n",
    "        torch.save(model.state_dict(), './model/3d_unet_postoperative/'+ model_name)\n",
    "        prev_val_loss = current_val_loss\n",
    "        eval_dsc.to_csv('./model/all_loss/3d_unet_postoperative_dsc_baysian.csv',index=False)\n",
    "        \n",
    "        print(\"Improved and model saved in \", epoch)\n",
    "    else:\n",
    "        print(\"No improvement in \", epoch)\n",
    "\n",
    "import gc\n",
    "del model\n",
    "gc.collect()\n",
    "with torch.cuda.device(device):\n",
    "    torch.cuda.empty_cache()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={}\n",
    "args['epoch']=30\n",
    "args['input_dim']=3\n",
    "args['kernal_size']=3\n",
    "args['slice_nu']=155\n",
    "args['window_size']=10\n",
    "args['pool_kernal_size']=2\n",
    "args['dilation_size']=1\n",
    "args['stride_size']=1\n",
    "args['pool_stride_size']=2\n",
    "args['padding_size']=1\n",
    "args['bias']=True\n",
    "args['act_type']='PReLU'\n",
    "args['norm_type']='GroupNorm'\n",
    "args['pool_type']='max'\n",
    "args['class_nu']=4\n",
    "args['batch_size']=2\n",
    "args['baysian']=True ; \n",
    "args['baysian_prior_mean']=0.0\n",
    "args['baysian_prior_variance']=1.0\n",
    "args['baysian_post_mu']=0\n",
    "args['baysian_post_rho']=-3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 16, 32]\n"
     ]
    }
   ],
   "source": [
    "en_channel=create_encoder_maps(4, 4)\n",
    "en_channel_r=en_channel.copy()\n",
    "en_channel_r.reverse()\n",
    "print(en_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[192, 96, 48]\n"
     ]
    }
   ],
   "source": [
    "de_channel=create_decoder_maps(4,en_channel_r)\n",
    "print(de_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_path='./pre_process_data/test/test_index.csv'\n",
    "test_df=pd.read_csv(test_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=14\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pt_id</th>\n",
       "      <th>images_path</th>\n",
       "      <th>seg_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34114852</td>\n",
       "      <td>./pre_process_data/test/images/34114852_images...</td>\n",
       "      <td>./pre_process_data/test/segmentation/34114852_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34169940</td>\n",
       "      <td>./pre_process_data/test/images/34169940_images...</td>\n",
       "      <td>./pre_process_data/test/segmentation/34169940_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34259182</td>\n",
       "      <td>./pre_process_data/test/images/34259182_images...</td>\n",
       "      <td>./pre_process_data/test/segmentation/34259182_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34290186</td>\n",
       "      <td>./pre_process_data/test/images/34290186_images...</td>\n",
       "      <td>./pre_process_data/test/segmentation/34290186_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35069039.2</td>\n",
       "      <td>./pre_process_data/test/images/35069039.2_imag...</td>\n",
       "      <td>./pre_process_data/test/segmentation/35069039....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pt_id                                        images_path  \\\n",
       "0    34114852  ./pre_process_data/test/images/34114852_images...   \n",
       "1    34169940  ./pre_process_data/test/images/34169940_images...   \n",
       "2    34259182  ./pre_process_data/test/images/34259182_images...   \n",
       "3    34290186  ./pre_process_data/test/images/34290186_images...   \n",
       "4  35069039.2  ./pre_process_data/test/images/35069039.2_imag...   \n",
       "\n",
       "                                            seg_path  \n",
       "0  ./pre_process_data/test/segmentation/34114852_...  \n",
       "1  ./pre_process_data/test/segmentation/34169940_...  \n",
       "2  ./pre_process_data/test/segmentation/34259182_...  \n",
       "3  ./pre_process_data/test/segmentation/34290186_...  \n",
       "4  ./pre_process_data/test/segmentation/35069039....  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpletest(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.data = dataset\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data_index = self.data.iloc[idx]\n",
    "        images_path=data_index.images_path\n",
    "        seg_path=data_index.seg_path\n",
    "        patient_id=data_index.pt_id\n",
    "        with open(images_path, 'rb') as f:\n",
    "            images = pickle.load(f)\n",
    "        \n",
    "        with open(seg_path, 'rb') as f:\n",
    "            seg = pickle.load(f)\n",
    "        \n",
    "        \n",
    "        return images, seg, str(patient_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=simpletest(test_df)\n",
    "test_loader=DataLoader(test_data,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3DUNet_postoperative_whole_model_baysian_low_3.pt', '3DUNet_postoperative_whole_model_baysian_low_5.pt', '3DUNet_postoperative_whole_model_baysian_low_7.pt']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path='./model/3d_unet_postoperative/'\n",
    "\n",
    "model_list=os.listdir(path)\n",
    "model_list=[i for i in model_list if 'baysian_low' in i ]\n",
    "model_list=[i for i in model_list if 'whole_model' in i ]\n",
    "model_list.sort()\n",
    "print(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_monte_carlo=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved example_segment_pred_34114852\n",
      "Saved example_segment_pred_34169940\n",
      "Saved example_segment_pred_34259182\n",
      "Saved example_segment_pred_34290186\n",
      "Saved example_segment_pred_35069039.2\n",
      "Saved example_segment_pred_35256564\n",
      "Saved example_segment_pred_35520522\n",
      "Saved example_segment_pred_36104836\n",
      "Saved example_segment_pred_36477034\n",
      "Saved example_segment_pred_38216722\n",
      "Saved example_segment_pred_39597056\n",
      "Saved example_segment_pred_39621144\n",
      "Saved example_segment_pred_39991662\n",
      "Saved example_segment_pred_40481407\n",
      "Saved example_segment_pred_40486118\n",
      "Saved example_segment_pred_40552846\n",
      "Saved example_segment_pred_41245644\n",
      "Saved example_segment_pred_41263165\n",
      "Saved example_segment_pred_41336169\n",
      "Saved example_segment_pred_45533978\n",
      "Saved example_segment_pred_45947450\n",
      "Saved example_segment_pred_46140963\n",
      "Saved example_segment_pred_46292941\n",
      "Saved example_segment_pred_46351381\n",
      "Saved example_segment_pred_46481193\n",
      "Saved example_segment_pred_46674661\n",
      "Saved example_segment_pred_47136867\n",
      "Saved example_segment_pred_47303976\n",
      "Saved example_segment_pred_47315835\n",
      "Saved example_segment_pred_47324599\n",
      "Saved example_segment_pred_61051121\n",
      "Saved example_segment_pred_85435283.2\n",
      "Saved example_segment_pred_J\n",
      "Saved example_segment_pred_K\n",
      "Saved example_segment_pred_L\n",
      "Saved example_segment_pred_M\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model_list)):\n",
    "    predict_folder=model_list[i].replace('.pt','')\n",
    "    isExist = os.path.exists('./Model_prediction/'+predict_folder)\n",
    "    if not isExist:\n",
    "        os.makedirs('./Model_prediction/'+predict_folder)\n",
    "    save_folder_path='./Model_prediction/'+predict_folder+'/'\n",
    "    en_channel_p=en_channel.copy()\n",
    "    encoder=UNet_3d_encoder(args,en_channel_p)\n",
    "    de_channel_p=de_channel.copy()\n",
    "    decoder=UNet_3d_decoder(args,de_channel_p)\n",
    "    en_channel_p=en_channel.copy()\n",
    "    encoder_pre=UNet_3d_encoder(args,en_channel_p)\n",
    "    de_channel_p=de_channel.copy()\n",
    "    decoder_pre=UNet_3d_decoder(args,de_channel_p)\n",
    "    model=Unet(encoder,decoder,encoder_pre,decoder_pre,args)\n",
    "    load_path='./model/3d_unet_postoperative/'+model_list[i]\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "    model.cuda()\n",
    "    for count, batch in enumerate(test_loader):\n",
    "        images, _, pt_id = batch\n",
    "        images=images.cuda()\n",
    "        seg_results = []\n",
    "        for j in range(num_monte_carlo):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output,_ = model(images)\n",
    "                seg_results.append(output.cpu())\n",
    "        seg_results= torch.stack(seg_results)   \n",
    "        seg_results=torch.squeeze(seg_results)\n",
    "        uncertainity_temp=np.var(seg_results.cpu().detach().numpy(), 0)\n",
    "        uncertainity_background=uncertainity_temp[0]\n",
    "        uncertainity_flair=uncertainity_temp[1]\n",
    "        uncertainity_con_enh=uncertainity_temp[2]\n",
    "        uncertainity_non_enh=uncertainity_temp[3]\n",
    "        uncertainity_all=uncertainity_temp.mean(0)\n",
    "        seg_layer_avg = torch.mean(seg_results, dim=0)\n",
    "        _, indices = seg_layer_avg.max(0)\n",
    "        indices = indices.cpu().detach().numpy().astype('uint8')\n",
    "        img = nib.Nifti1Image(indices, [[-1.,  0.,  0., -0.],[0.,  1.,  0., -0.],[0.,  0.,  1.,  0.],[0.,  0.,  0.,  1.]])\n",
    "        uncertainity_img_back = nib.Nifti1Image(uncertainity_background, [[-1.,  0.,  0., -0.],[0.,  1.,  0., -0.],[0.,  0.,  1.,  0.],[0.,  0.,  0.,  1.]])\n",
    "        uncertainity_img_flair = nib.Nifti1Image(uncertainity_flair, [[-1.,  0.,  0., -0.],[0.,  1.,  0., -0.],[0.,  0.,  1.,  0.],[0.,  0.,  0.,  1.]])\n",
    "        uncertainity_img_contrast = nib.Nifti1Image(uncertainity_con_enh, [[-1.,  0.,  0., -0.],[0.,  1.,  0., -0.],[0.,  0.,  1.,  0.],[0.,  0.,  0.,  1.]])\n",
    "        uncertainity_img_non_enh = nib.Nifti1Image(uncertainity_non_enh, [[-1.,  0.,  0., -0.],[0.,  1.,  0., -0.],[0.,  0.,  1.,  0.],[0.,  0.,  0.,  1.]])\n",
    "        uncertainity_img_all = nib.Nifti1Image(uncertainity_all, [[-1.,  0.,  0., -0.],[0.,  1.,  0., -0.],[0.,  0.,  1.,  0.],[0.,  0.,  0.,  1.]])\n",
    "        nib.save(img, save_folder_path+str(pt_id[0])+'_segment_pred_kg.nii.gz')\n",
    "        nib.save(uncertainity_img_back, save_folder_path+str(pt_id[0])+'_segment_pred_new_kg_uncertainity_back.nii.gz')\n",
    "        nib.save(uncertainity_img_flair, save_folder_path+str(pt_id[0])+'_segment_pred_new_kg_uncertainity_flair.nii.gz')\n",
    "        nib.save(uncertainity_img_contrast, save_folder_path+str(pt_id[0])+'_segment_pred_new_kg_uncertainity_con.nii.gz')\n",
    "        nib.save(uncertainity_img_non_enh, save_folder_path+str(pt_id[0])+'_segment_pred_new_kg_uncertainity_non_enh.nii.gz')\n",
    "        nib.save(uncertainity_img_all, save_folder_path+str(pt_id[0])+'_segment_pred_new_kg_uncertainity_all.nii.gz')\n",
    "        print('Saved example_segment_pred'+'_'+str(pt_id[0]))\n",
    "    del model\n",
    "    gc.collect()\n",
    "    with torch.cuda.device(device):\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "with torch.cuda.device(device):\n",
    "    torch.cuda.empty_cache()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
